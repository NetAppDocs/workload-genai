---
sidebar: sidebar
permalink: knowledge-base/create-knowledgebase.html
keywords: ai, chatbot, creating, knowledge base
summary: After you've deployed the AI infrastructure and identified the data sources that you'll integrate in your knowledge base from your FSx for ONTAP datastores, you are ready to build the knowledge base using Workload Factory. As part of this step you'll also define the AI characteristics and create conversation starters.
---

= Create a GenAI knowledge base
:icons: font
:imagesdir: ../media/

[.lead]
After you've deployed the AI infrastructure and identified the data sources that you'll integrate in your knowledge base from your FSx for ONTAP datastores, you are ready to build the knowledge base using Workload Factory. As part of this step, you'll also define the AI characteristics and create conversation starters.

Ensure that your environment meets the link:requirements-knowledge-base.html[requirements] for knowledge bases before proceeding.

.About this task
Knowledge bases have two data integration modalities - _public mode_ and _Enterprise mode_. 

Public mode:: A knowledge base can be used without integrating data sources from your organization. In this case, an application integrated with the knowledge base will only provide results from publicly available information on the internet. This is known as a _public mode_ integration.

Enterprise mode:: In most cases you'll want to integrate data sources from your organization into the knowledge base. This is known as an _Enterprise mode_ integration because it provides knowledge from your enterprise.
+
Data sources from your organization may contain Personally Identifiable Information (PII). To safeguard this sensitive information, you can enable _data guardrails_ when creating and configuring knowledge bases. Data guardrails, powered by NetApp Data Classification, identifies and masks PII, making it inaccessible and irretrievable.
+
link:https://docs.netapp.com/us-en/bluexp-classification/concept-cloud-compliance.html[Learn about NetApp Data Classification^].
+
NOTE: NetApp Workload Factory for GenAI does not mask sensitive personal information (SPii). Refer to link:https://docs.netapp.com/us-en/bluexp-classification/reference-private-data-categories.html#types-of-sensitive-personal-data[types of sensitive personal data^] for more information about this type of data.
+
NOTE: Data guardrails can be enabled or disabled at any time. If you switch data guardrails enablement, Workload Factory scans the entire knowledge base from scratch, which incurs a cost.  

== Create and configure the knowledge base

The knowledge base defines characteristics such as the Bedrock AI models and embedding format that you want to use to create your knowledge base.

.Steps

. Log in to Workload Factory using one of the link:https://docs.netapp.com/us-en/workload-setup-admin/console-experiences.html[console experiences^].

. In the AI workloads tile, select *Deploy & manage*. 

. From the Knowledge bases & Connectors menu, select the *Create New* dropdown and choose *NetApp GenAI knowledge base for Bedrock*.

. On the Create NetApp GenAI knowledge base page, configure the knowledge base settings:

[discrete]
===== Knowledge base details

.. *Name*: Enter the name you want to use for the knowledge base.
.. *Description*: Enter a detailed description for the knowledge base.
.. *Bedrock*: Choose the region where Amazon Bedrock is available for your AWS account.

[discrete]
===== Ingestion

.. *Embedding model*: 
+
* Choose an embedding model to use for the knowledge base. The embedding model defines how your data will be converted into vector embeddings for the knowledge base. Workload Factory supports the following models:
+
* Titan Embeddings G1 - Text
* Titan Embedding Text v2
* Titan Multimodal Embeddings G1
* Embed English
* Embed Multilingual
+
Note that you must have already enabled the embedding model from Amazon Bedrock.
+
https://aws.amazon.com/bedrock/titan/[Learn more about Amazon Titan^]
+
* If applicable, select the inference type that matches the configuration of the selected embedding model.

.. *Data guardrails*: Choose whether you want to enable or disable data guardrails. link:https://docs.netapp.com/us-en/bluexp-classification/concept-cloud-compliance.html[Learn about data guardrails, powered by NetApp Data Classification^].
+
The following prerequisites must be met to enable data guardrails. 
+
* A service account is required to communicate with NetApp Data Classification. You must have the _Organization admin_ role on your NetApp Console tenancy account for service account creation. A member who has the Organization admin role can complete all actions in the . link:https://docs.netapp.com/us-en/bluexp-setup-admin/task-iam-manage-members-permissions.html#add-a-role-to-a-member[Learn how to add a role to a member in the NetApp Console^]
+
* The AI engine must have access to the link:https://api.bluexp.netapp.com[NetApp Console API endpoint^].
+ 
* You'll need to do the following as described in link:https://docs.netapp.com/us-en/bluexp-classification/task-deploy-cloud-compliance.html#quick-start[NetApp Data Classification documentation^]: 
+
1. Create a Console agent
2. Ensure that your environment can meet the prerequisites
3. Deploy NetApp Data Classification

+
NOTE: The data guardrails feature is not supported when ingesting structured data files such as CSV, JSON, JSONP, or Parquet.

[discrete]
===== Chat and Retrieval settings

.. *Chat model*:
+
* Choose from various chat models that are integrated in Amazon Bedrock. Note that you must have already enabled the chat model from Amazon Bedrock.
* If applicable, select the inference type that matches the configuration of the selected model.

.. *Chat settings*: 
+
* Choose a temperature for the chatbot to configure the randomness and creativity of responses. A lower temperature results in more predictable responses, and a higher temperature results in more varied responses.
* Choose a maximum response length to configure how detailed responses are. Longer response lengths use more response tokens, and can incur a higher cost.

.. *Thinking mode*: When thinking mode is enabled, the chatbot will take more time to process queries and the results will usually be more accurate. When you enable thinking mode, you can control how many reasoning tokens are used when generating results. Using more reasoning tokens can lead to more accurate responses, but might incur a higher cost.

.. *Reranking*: Enable or disable reranking, which can improve the relevance and quality of query results. Choose a standard chat model or a specialized reranker model to use for reranking. Reranker model options are only shown if they are available in your region. Select the inference type that matches the configuration of the selected model.

.. *Conversation starters*: Choose whether you want to provide up to four conversation starter prompts that are displayed to users who interact with a chatbot that uses this knowledge base. We recommend that you enable this setting.
+
If you activate conversation starters, "Automatic mode" is selected by default. "Manual mode" can be enabled only after you've added data sources to your knowledge base. link:manage-knowledgebase.html[Learn how to modify knowledge base settings].

//.. *System prompt*: Enter text that will be displayed to the user as soon as the chat session begins.

////
.. *Retrieval settings*:
+
* Set a *Minimum reranking score* (available only if reranking is enabled) to configure the ordering of query results based on the reranking score. GenAI only includes results that have a score equal to or higher than the value you set.
* Choose the number of *Maximum database chunks* to configure the maximum number of chunks to use from the database for semantic and textual searches. The default value is 5 for each search.
* Choose the number of *Maximum total chunks* to configure the maximum number of chunks to return at the end of the retrieval process.
* Choose the *Maximum distance* to configure the exclusion of irrelevant results from a semantic search. GenAI only includes results with a distance smaller than the value you set. A smaller distance instructs GenAI to only include results with a higher similarity to the query. By default, GenAI includes results with a distance of less than 0.5.
* Select a *Search type* to configure the chatbot to use semantic search (relying on meaning and context of queries) or hybrid search (keyword search combined with semantic search) when generating results. Hybrid search is the default setting.

.. *Retrieval time configuration*:
+
* Enable recency bias to instruct the chatbot to prefer more recent information from data sources when formulating a response.
* Configure a time filter for document retrieval to limit the age of documents that the chatbot retrieves from data sources. Documents older than the time filter will not appear in results.

////

[discrete]
===== Storage definitions

.. *FSx for ONTAP file system*: When you define a new knowledge base, Workload Factory creates a new Amazon FSx for NetApp ONTAP volume to store it. Choose an existing file system name and SVM (also called a storage VM) where the new volume will be created.
.. *Snapshot policy*: Choose a snapshot policy from the list of existing policies defined in the Workload Factory storage inventory. Recurring snapshots of the knowledge base will automatically be created at a frequency based on the snapshot policy you select.
.. *S3 Bucket*: If chatbot query results contain structured data, GenAI can store the results in an S3 bucket. To use this feature, enable the *Activate S3 Bucket* setting and choose an S3 bucket that is associated with your account from the list. When these results are stored in an S3 bucket, you can download them using the download link within the chat session.
+
If the snapshot policy you need doesn't exist, you can https://docs.netapp.com/us-en/ontap/data-protection/create-snapshot-policy-task.html[create a snapshot policy] on the storage VM that contains the volume.

[start=5]
. Select *Create knowledge base* to add the knowledge base to GenAI.
+
A progress indicator appears while the knowledge base is created.
+
After the knowledge base is created, you have the option to add a data source to your new knowledge base or to end the process without adding a data source. We recommend that you select *Add data source* and add one or more data sources now.

== Add data sources to the knowledge base

You can add one or more data sources to populate the knowledge base with your organization's data.

//NOTE: If you enabled data guardrails for the knowledge base, make sure you set the chunk size to #/between # and # so that Personal Identifiable Information (PII) can be found.
//add after Chen provides range

.About this task

The maximum number of supported data sources is 10.

.Steps

. After you select *Add data source*, select the type of data source you want to add:
+
* Add FSx for ONTAP file system (use files from an existing FSx for ONTAP volume)
* Add file system (use files from a generic SMB or NFS share)

include::_include/add-data-source-kb.adoc[]

.Result

The data source starts to be embedded into your knowledge base. The status changes from "Embedding" to "Embedded" when the data source is completely embedded.

After you add a single data source to the knowledge base, you can test it locally in the chatbot simulator window and make any required changes before you make the chatbot available to your users. You can also follow the same steps to add additional data sources to the knowledge base.